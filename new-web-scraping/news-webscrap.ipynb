{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6159cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.robotparser import RobotFileParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "056d5f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- CONFIG ----------\n",
    "# Use a realistic browser UA for news sites\n",
    "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "DEFAULT_DELAY= 2.0    # minimum seconds between requests (randomized)\n",
    "DEFAULT_DELAY_MAX = 5.0    # maximum seconds between requests (randomized)\n",
    "TIMEOUT = (10, 40)         # (connect timeout, read timeout) in seconds\n",
    "MAX_RETRIES = 4            # network retries\n",
    "BACKOFF_FACTOR = 1.0       # exponential backoff factor for retries\n",
    "# Optionally rotate a small list of UAs to be safer (not required)\n",
    "USER_AGENTS = [USER_AGENT]\n",
    "# ---------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f72f0d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] https://indianexpress.com/ (words: 4066)\n",
      "[2] https://indianexpress.com/search/ (words: 1397)\n",
      "[3] https://indianexpress.com/newsletters/ (words: 218)\n",
      "[4] https://indianexpress.com/international/ (words: 536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[fetch error] https://indianexpress.com/subscribe/all-access/ -> 403 Client Error: Forbidden for url: https://indianexpress.com/subscribe/all-access/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] https://indianexpress.com/todays-paper/ (words: 264)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[fetch error] https://indianexpress.com/subscribe/all-access/?utm_source=IESITE&utm_medium=Banner&utm_campaign=EpaperL1 -> 403 Client Error: Forbidden for url: https://indianexpress.com/subscribe/all-access/?utm_source=IESITE&utm_medium=Banner&utm_campaign=EpaperL1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] https://indianexpress.com/section/india/ (words: 2153)\n",
      "[7] https://indianexpress.com/section/upsc-current-affairs/ (words: 2770)\n",
      "[8] https://indianexpress.com/about/express-premium/ (words: 1378)\n",
      "[9] https://indianexpress.com/section/entertainment/ (words: 32)\n",
      "[10] https://indianexpress.com/politics/ (words: 804)\n",
      "Saved JSON -> scrape_outputs/indianexp_output.json\n",
      "Saved CSV -> scrape_outputs/indianexp_output.csv\n"
     ]
    }
   ],
   "source": [
    "def allowed_by_robots(start_url, user_agent=USER_AGENT):\n",
    "    parsed = urlparse(start_url)\n",
    "    robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "    rp = RobotFileParser()\n",
    "    try:\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        return rp\n",
    "    except Exception:\n",
    "        # if we cannot fetch robots.txt, return a permissive parser (but still be polite)\n",
    "        rp = RobotFileParser()\n",
    "        rp.parse(\"\")  # empty rules -> allow?\n",
    "        return rp\n",
    "\n",
    "def get_domain_and_prefix(start_url):\n",
    "    p = urlparse(start_url)\n",
    "    domain = f\"{p.scheme}://{p.netloc}\"\n",
    "    prefix = p.path.rsplit(\"/\", 1)[0]  # parent path (useful if you want to restrict)\n",
    "    return domain, prefix\n",
    "\n",
    "def is_internal_link(link, base_netloc):\n",
    "    if not link:\n",
    "        return False\n",
    "    parsed = urlparse(link)\n",
    "    if parsed.netloc == \"\" or parsed.netloc == base_netloc:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def normalize_url(href, base_url):\n",
    "    if not href:\n",
    "        return None\n",
    "    href = href.strip()\n",
    "    # ignore javascript:, mailto:, tel:\n",
    "    if href.startswith(\"javascript:\") or href.startswith(\"mailto:\") or href.startswith(\"tel:\") or href.startswith(\"#\"):\n",
    "        return None\n",
    "    return urljoin(base_url, href.split(\"#\")[0])  # remove fragment\n",
    "\n",
    "def extract_main_text(soup):\n",
    "    # Heuristics: prefer <article>, then largest <div> or <main>, then body text.\n",
    "    article = soup.find(\"article\")\n",
    "    if article and len(article.get_text(strip=True)) > 100:\n",
    "        return article.get_text(separator=\"\\n\", strip=True)\n",
    "    main = soup.find(\"main\")\n",
    "    if main and len(main.get_text(strip=True)) > 100:\n",
    "        return main.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # find the largest div by text length\n",
    "    divs = soup.find_all(\"div\")\n",
    "    best = \"\"\n",
    "    for d in divs:\n",
    "        txt = d.get_text(separator=\"\\n\", strip=True)\n",
    "        if len(txt) > len(best):\n",
    "            best = txt\n",
    "    if len(best) > 50:\n",
    "        return best\n",
    "\n",
    "    # fallback: full body text\n",
    "    body = soup.body\n",
    "    return body.get_text(separator=\"\\n\", strip=True) if body else soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "def fetch_page(session, url, user_agent=USER_AGENT):\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    resp = session.get(url, headers=headers, timeout=TIMEOUT)\n",
    "    resp.raise_for_status()\n",
    "    return resp\n",
    "\n",
    "def crawl(start_url, max_pages=200, delay=DEFAULT_DELAY, restrict_to_prefix=True, output_formats=(\"json\",), out_file=\"output\"):\n",
    "    parsed_start = urlparse(start_url)\n",
    "    base_domain = parsed_start.netloc\n",
    "    base_origin = f\"{parsed_start.scheme}://{parsed_start.netloc}\"\n",
    "    prefix_path = parsed_start.path\n",
    "    if restrict_to_prefix:\n",
    "        prefix_path = prefix_path.split(\"/\", 2)[:2]  # keep safe prefix (approx)\n",
    "    else:\n",
    "        prefix_path = None\n",
    "\n",
    "    rp = allowed_by_robots(base_origin)\n",
    "    queue = deque([start_url])\n",
    "    seen = set([start_url])\n",
    "    results = []\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    pages_crawled = 0\n",
    "    while queue and pages_crawled < max_pages:\n",
    "        url = queue.popleft()\n",
    "        # robots check\n",
    "        try:\n",
    "            can_fetch = rp.can_fetch(USER_AGENT, url)\n",
    "        except Exception:\n",
    "            can_fetch = True\n",
    "        if not can_fetch:\n",
    "            print(f\"[robots.txt blocked] {url}\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            resp = fetch_page(session, url)\n",
    "        except Exception as e:\n",
    "            print(f\"[fetch error] {url} -> {e}\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        title = soup.title.string.strip() if soup.title and soup.title.string else \"\"\n",
    "        meta_desc = \"\"\n",
    "        md = soup.find(\"meta\", attrs={\"name\": re.compile(r\"description\", re.I)})\n",
    "        if md and md.get(\"content\"):\n",
    "            meta_desc = md.get(\"content\").strip()\n",
    "        else:\n",
    "            og_desc = soup.find(\"meta\", property=\"og:description\")\n",
    "            if og_desc and og_desc.get(\"content\"):\n",
    "                meta_desc = og_desc.get(\"content\").strip()\n",
    "\n",
    "        main_text = extract_main_text(soup)\n",
    "        links = []\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a.get(\"href\")\n",
    "            nu = normalize_url(href, url)\n",
    "            if not nu:\n",
    "                continue\n",
    "            parsed = urlparse(nu)\n",
    "            if parsed.scheme not in (\"http\", \"https\"):\n",
    "                continue\n",
    "            # only internal links\n",
    "            if parsed.netloc != base_domain:\n",
    "                # still collect outbound links but don't queue them\n",
    "                links.append({\"url\": nu, \"internal\": False})\n",
    "                continue\n",
    "            links.append({\"url\": nu, \"internal\": True})\n",
    "            # optionally restrict to path prefix to avoid entire domain crawl\n",
    "            if nu not in seen:\n",
    "                if restrict_to_prefix:\n",
    "                    # ensure starts with the same top-level path (optional)\n",
    "                    if urlparse(nu).path.startswith(urlparse(start_url).path.split(\"/\",2)[1] if \"/\" in urlparse(start_url).path[1:] else \"/\"):\n",
    "                        queue.append(nu)\n",
    "                        seen.add(nu)\n",
    "                else:\n",
    "                    queue.append(nu)\n",
    "                    seen.add(nu)\n",
    "\n",
    "        result = {\n",
    "            \"url\": url,\n",
    "            \"status_code\": resp.status_code,\n",
    "            \"title\": title,\n",
    "            \"meta_description\": meta_desc,\n",
    "            \"content\": main_text,\n",
    "            \"num_words\": len(main_text.split()),\n",
    "            \"links\": links,\n",
    "        }\n",
    "        results.append(result)\n",
    "        pages_crawled += 1\n",
    "        print(f\"[{pages_crawled}] {url} (words: {result['num_words']})\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "    # Save outputs\n",
    "    os.makedirs(\"scrape_outputs\", exist_ok=True)\n",
    "\n",
    "    if \"json\" in output_formats or \"all\" in output_formats:\n",
    "        json_path = f\"scrape_outputs/{out_file}.json\"\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved JSON -> {json_path}\")\n",
    "\n",
    "    if \"csv\" in output_formats or \"all\" in output_formats:\n",
    "        csv_path = f\"scrape_outputs/{out_file}.csv\"\n",
    "        with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"url\", \"status_code\", \"title\", \"meta_description\", \"num_words\", \"content_snippet\"])\n",
    "            for r in results:\n",
    "                snippet = (r[\"content\"][:300] + \"...\") if len(r[\"content\"]) > 300 else r[\"content\"]\n",
    "                writer.writerow([r[\"url\"], r[\"status_code\"], r[\"title\"], r[\"meta_description\"], r[\"num_words\"], snippet])\n",
    "        print(f\"Saved CSV -> {csv_path}\")\n",
    "\n",
    "    if \"xml\" in output_formats or \"all\" in output_formats:\n",
    "        xml_path = f\"scrape_outputs/{out_file}.xml\"\n",
    "        root = ET.Element(\"pages\")\n",
    "        for r in results:\n",
    "            p = ET.SubElement(root, \"page\")\n",
    "            ET.SubElement(p, \"url\").text = r[\"url\"]\n",
    "            ET.SubElement(p, \"status_code\").text = str(r[\"status_code\"])\n",
    "            ET.SubElement(p, \"title\").text = r[\"title\"] or \"\"\n",
    "            ET.SubElement(p, \"meta_description\").text = r[\"meta_description\"] or \"\"\n",
    "            ET.SubElement(p, \"num_words\").text = str(r[\"num_words\"])\n",
    "            c = ET.SubElement(p, \"content\")\n",
    "            c.text = r[\"content\"]\n",
    "        tree = ET.ElementTree(root)\n",
    "        tree.write(xml_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "        print(f\"Saved XML -> {xml_path}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Multipage scraper (requests + BeautifulSoup)\")\n",
    "    p.add_argument(\"start_url\", help=\"Starting URL (e.g. https://www.example.com/some/page)\")\n",
    "    p.add_argument(\"--max-pages\", type=int, default=200, help=\"Maximum pages to crawl\")\n",
    "    p.add_argument(\"--delay\", type=float, default=DEFAULT_DELAY, help=\"Delay between requests in seconds\")\n",
    "    p.add_argument(\"--output\", choices=[\"json\", \"csv\", \"xml\", \"all\"], default=\"json\", help=\"Output format(s)\")\n",
    "    p.add_argument(\"--out-file\", default=\"output\", help=\"Base filename (without extension) saved in scrape_outputs/\")\n",
    "    p.add_argument(\"--no-prefix-restrict\", action=\"store_true\", help=\"Don't restrict crawl to same path prefix (will crawl entire domain)\")\n",
    "    return p.parse_args()\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # If running inside Jupyter/IPython, ignore unknown args\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        results = crawl(\n",
    "            start_url=\"https://indianexpress.com/\",\n",
    "            max_pages=10,\n",
    "            delay=1.0,\n",
    "            restrict_to_prefix=True,\n",
    "            output_formats=(\"json\", \"csv\"),\n",
    "            out_file=\"indianexp_output\"\n",
    "        )\n",
    "    else:\n",
    "        args = parse_args()\n",
    "        out_formats = (args.output,) if args.output != \"all\" else (\"json\", \"csv\", \"xml\")\n",
    "        crawl(\n",
    "            start_url=args.start_url,\n",
    "            max_pages=args.max_pages,\n",
    "            delay=args.delay,\n",
    "            restrict_to_prefix=(not args.no_prefix_restrict),\n",
    "            output_formats=out_formats,\n",
    "            out_file=args.out_file,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
