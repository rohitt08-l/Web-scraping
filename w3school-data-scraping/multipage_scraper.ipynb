{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a39afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmultipage_scraper.py\\n\\nUsage examples:\\n    # Basic run (scrape up to 200 pages starting from the given URL)\\n    python multipage_scraper.py \"https://www.w3schools.com/html/html_responsive.asp\" --max-pages 200 --output json --out-file w3_pages.json\\n\\n    # Save to all formats and limit to 50 pages with 1 second delay\\n    python multipage_scraper.py \"https://www.w3schools.com/html/html_responsive.asp\" --max-pages 50 --output all --delay 1.0 --out-file w3_all\\n\\nNotes:\\n - The scraper respects robots.txt by default.\\n - It only follows links within the same domain and (optionally) same path prefix.\\n - Be polite: set delay and don\\'t hammer the server.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "multipage_scraper.py\n",
    "\n",
    "Usage examples:\n",
    "    # Basic run (scrape up to 200 pages starting from the given URL)\n",
    "    python multipage_scraper.py \"https://www.w3schools.com/html/html_responsive.asp\" --max-pages 200 --output json --out-file w3_pages.json\n",
    "\n",
    "    # Save to all formats and limit to 50 pages with 1 second delay\n",
    "    python multipage_scraper.py \"https://www.w3schools.com/html/html_responsive.asp\" --max-pages 50 --output all --delay 1.0 --out-file w3_all\n",
    "\n",
    "Notes:\n",
    " - The scraper respects robots.txt by default.\n",
    " - It only follows links within the same domain and (optionally) same path prefix.\n",
    " - Be polite: set delay and don't hammer the server.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543fa810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.robotparser import RobotFileParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96e29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- CONFIG ----------\n",
    "USER_AGENT = \"MultipageScraper/1.0 (+https://yourdomain.example)\"\n",
    "DEFAULT_DELAY = 0.5  # seconds between requests\n",
    "TIMEOUT = 15\n",
    "# ---------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c409f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] https://www.w3schools.com/html/html_responsive.asp (words: 1209)\n",
      "Saved JSON -> scrape_outputs/w3_output.json\n",
      "Saved CSV -> scrape_outputs/w3_output.csv\n"
     ]
    }
   ],
   "source": [
    "def allowed_by_robots(start_url, user_agent=USER_AGENT):\n",
    "    parsed = urlparse(start_url)\n",
    "    robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "    rp = RobotFileParser()\n",
    "    try:\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        return rp\n",
    "    except Exception:\n",
    "        # if we cannot fetch robots.txt, return a permissive parser (but still be polite)\n",
    "        rp = RobotFileParser()\n",
    "        rp.parse(\"\")  # empty rules -> allow?\n",
    "        return rp\n",
    "\n",
    "def get_domain_and_prefix(start_url):\n",
    "    p = urlparse(start_url)\n",
    "    domain = f\"{p.scheme}://{p.netloc}\"\n",
    "    prefix = p.path.rsplit(\"/\", 1)[0]  # parent path (useful if you want to restrict)\n",
    "    return domain, prefix\n",
    "\n",
    "def is_internal_link(link, base_netloc):\n",
    "    if not link:\n",
    "        return False\n",
    "    parsed = urlparse(link)\n",
    "    if parsed.netloc == \"\" or parsed.netloc == base_netloc:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def normalize_url(href, base_url):\n",
    "    if not href:\n",
    "        return None\n",
    "    href = href.strip()\n",
    "    # ignore javascript:, mailto:, tel:\n",
    "    if href.startswith(\"javascript:\") or href.startswith(\"mailto:\") or href.startswith(\"tel:\") or href.startswith(\"#\"):\n",
    "        return None\n",
    "    return urljoin(base_url, href.split(\"#\")[0])  # remove fragment\n",
    "\n",
    "def extract_main_text(soup):\n",
    "    # Heuristics: prefer <article>, then largest <div> or <main>, then body text.\n",
    "    article = soup.find(\"article\")\n",
    "    if article and len(article.get_text(strip=True)) > 100:\n",
    "        return article.get_text(separator=\"\\n\", strip=True)\n",
    "    main = soup.find(\"main\")\n",
    "    if main and len(main.get_text(strip=True)) > 100:\n",
    "        return main.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # find the largest div by text length\n",
    "    divs = soup.find_all(\"div\")\n",
    "    best = \"\"\n",
    "    for d in divs:\n",
    "        txt = d.get_text(separator=\"\\n\", strip=True)\n",
    "        if len(txt) > len(best):\n",
    "            best = txt\n",
    "    if len(best) > 50:\n",
    "        return best\n",
    "\n",
    "    # fallback: full body text\n",
    "    body = soup.body\n",
    "    return body.get_text(separator=\"\\n\", strip=True) if body else soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "def fetch_page(session, url, user_agent=USER_AGENT):\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    resp = session.get(url, headers=headers, timeout=TIMEOUT)\n",
    "    resp.raise_for_status()\n",
    "    return resp\n",
    "\n",
    "def crawl(start_url, max_pages=200, delay=DEFAULT_DELAY, restrict_to_prefix=True, output_formats=(\"json\",), out_file=\"output\"):\n",
    "    parsed_start = urlparse(start_url)\n",
    "    base_domain = parsed_start.netloc\n",
    "    base_origin = f\"{parsed_start.scheme}://{parsed_start.netloc}\"\n",
    "    prefix_path = parsed_start.path\n",
    "    if restrict_to_prefix:\n",
    "        prefix_path = prefix_path.split(\"/\", 2)[:2]  # keep safe prefix (approx)\n",
    "    else:\n",
    "        prefix_path = None\n",
    "\n",
    "    rp = allowed_by_robots(base_origin)\n",
    "    queue = deque([start_url])\n",
    "    seen = set([start_url])\n",
    "    results = []\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    pages_crawled = 0\n",
    "    while queue and pages_crawled < max_pages:\n",
    "        url = queue.popleft()\n",
    "        # robots check\n",
    "        try:\n",
    "            can_fetch = rp.can_fetch(USER_AGENT, url)\n",
    "        except Exception:\n",
    "            can_fetch = True\n",
    "        if not can_fetch:\n",
    "            print(f\"[robots.txt blocked] {url}\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            resp = fetch_page(session, url)\n",
    "        except Exception as e:\n",
    "            print(f\"[fetch error] {url} -> {e}\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        title = soup.title.string.strip() if soup.title and soup.title.string else \"\"\n",
    "        meta_desc = \"\"\n",
    "        md = soup.find(\"meta\", attrs={\"name\": re.compile(r\"description\", re.I)})\n",
    "        if md and md.get(\"content\"):\n",
    "            meta_desc = md.get(\"content\").strip()\n",
    "        else:\n",
    "            og_desc = soup.find(\"meta\", property=\"og:description\")\n",
    "            if og_desc and og_desc.get(\"content\"):\n",
    "                meta_desc = og_desc.get(\"content\").strip()\n",
    "\n",
    "        main_text = extract_main_text(soup)\n",
    "        links = []\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a.get(\"href\")\n",
    "            nu = normalize_url(href, url)\n",
    "            if not nu:\n",
    "                continue\n",
    "            parsed = urlparse(nu)\n",
    "            if parsed.scheme not in (\"http\", \"https\"):\n",
    "                continue\n",
    "            # only internal links\n",
    "            if parsed.netloc != base_domain:\n",
    "                # still collect outbound links but don't queue them\n",
    "                links.append({\"url\": nu, \"internal\": False})\n",
    "                continue\n",
    "            links.append({\"url\": nu, \"internal\": True})\n",
    "            # optionally restrict to path prefix to avoid entire domain crawl\n",
    "            if nu not in seen:\n",
    "                if restrict_to_prefix:\n",
    "                    # ensure starts with the same top-level path (optional)\n",
    "                    if urlparse(nu).path.startswith(urlparse(start_url).path.split(\"/\",2)[1] if \"/\" in urlparse(start_url).path[1:] else \"/\"):\n",
    "                        queue.append(nu)\n",
    "                        seen.add(nu)\n",
    "                else:\n",
    "                    queue.append(nu)\n",
    "                    seen.add(nu)\n",
    "\n",
    "        result = {\n",
    "            \"url\": url,\n",
    "            \"status_code\": resp.status_code,\n",
    "            \"title\": title,\n",
    "            \"meta_description\": meta_desc,\n",
    "            \"content\": main_text,\n",
    "            \"num_words\": len(main_text.split()),\n",
    "            \"links\": links,\n",
    "        }\n",
    "        results.append(result)\n",
    "        pages_crawled += 1\n",
    "        print(f\"[{pages_crawled}] {url} (words: {result['num_words']})\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "    # Save outputs\n",
    "    os.makedirs(\"scrape_outputs\", exist_ok=True)\n",
    "\n",
    "    if \"json\" in output_formats or \"all\" in output_formats:\n",
    "        json_path = f\"scrape_outputs/{out_file}.json\"\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved JSON -> {json_path}\")\n",
    "\n",
    "    if \"csv\" in output_formats or \"all\" in output_formats:\n",
    "        csv_path = f\"scrape_outputs/{out_file}.csv\"\n",
    "        with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"url\", \"status_code\", \"title\", \"meta_description\", \"num_words\", \"content_snippet\"])\n",
    "            for r in results:\n",
    "                snippet = (r[\"content\"][:300] + \"...\") if len(r[\"content\"]) > 300 else r[\"content\"]\n",
    "                writer.writerow([r[\"url\"], r[\"status_code\"], r[\"title\"], r[\"meta_description\"], r[\"num_words\"], snippet])\n",
    "        print(f\"Saved CSV -> {csv_path}\")\n",
    "\n",
    "    if \"xml\" in output_formats or \"all\" in output_formats:\n",
    "        xml_path = f\"scrape_outputs/{out_file}.xml\"\n",
    "        root = ET.Element(\"pages\")\n",
    "        for r in results:\n",
    "            p = ET.SubElement(root, \"page\")\n",
    "            ET.SubElement(p, \"url\").text = r[\"url\"]\n",
    "            ET.SubElement(p, \"status_code\").text = str(r[\"status_code\"])\n",
    "            ET.SubElement(p, \"title\").text = r[\"title\"] or \"\"\n",
    "            ET.SubElement(p, \"meta_description\").text = r[\"meta_description\"] or \"\"\n",
    "            ET.SubElement(p, \"num_words\").text = str(r[\"num_words\"])\n",
    "            c = ET.SubElement(p, \"content\")\n",
    "            c.text = r[\"content\"]\n",
    "        tree = ET.ElementTree(root)\n",
    "        tree.write(xml_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "        print(f\"Saved XML -> {xml_path}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser(description=\"Multipage scraper (requests + BeautifulSoup)\")\n",
    "    p.add_argument(\"start_url\", help=\"Starting URL (e.g. https://www.example.com/some/page)\")\n",
    "    p.add_argument(\"--max-pages\", type=int, default=200, help=\"Maximum pages to crawl\")\n",
    "    p.add_argument(\"--delay\", type=float, default=DEFAULT_DELAY, help=\"Delay between requests in seconds\")\n",
    "    p.add_argument(\"--output\", choices=[\"json\", \"csv\", \"xml\", \"all\"], default=\"json\", help=\"Output format(s)\")\n",
    "    p.add_argument(\"--out-file\", default=\"output\", help=\"Base filename (without extension) saved in scrape_outputs/\")\n",
    "    p.add_argument(\"--no-prefix-restrict\", action=\"store_true\", help=\"Don't restrict crawl to same path prefix (will crawl entire domain)\")\n",
    "    return p.parse_args()\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # If running inside Jupyter/IPython, ignore unknown args\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        results = crawl(\n",
    "            start_url=\"https://www.w3schools.com/html/html_responsive.asp\",\n",
    "            max_pages=10,\n",
    "            delay=1.0,\n",
    "            restrict_to_prefix=True,\n",
    "            output_formats=(\"json\", \"csv\"),\n",
    "            out_file=\"w3_output\"\n",
    "        )\n",
    "    else:\n",
    "        args = parse_args()\n",
    "        out_formats = (args.output,) if args.output != \"all\" else (\"json\", \"csv\", \"xml\")\n",
    "        crawl(\n",
    "            start_url=args.start_url,\n",
    "            max_pages=args.max_pages,\n",
    "            delay=args.delay,\n",
    "            restrict_to_prefix=(not args.no_prefix_restrict),\n",
    "            output_formats=out_formats,\n",
    "            out_file=args.out_file,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc135486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
